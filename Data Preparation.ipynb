{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Preparation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbtW6BfB1td1",
        "outputId": "2c8cfd51-d163-464a-891c-345b1e7e651b"
      },
      "source": [
        "!pip install ipython-autotime > /dev/null\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 219 µs (started: 2021-09-03 18:29:47 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnAV5-gRZTEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f49b6e4-5580-4f37-b8cd-57770f8d2b19"
      },
      "source": [
        "!pip install -U datasets > /dev/null\n",
        "!pip install contractions > /dev/null\n",
        "!pip install -U tqdm > /dev/null\n",
        "!pip install fasttext > /dev/null"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1min 2s (started: 2021-09-03 18:29:47 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSi0WIR05tY4",
        "outputId": "d90cc38a-8a42-4ea7-a91b-acde2c4a6753"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"drive/MyDrive/Dev/ID_in_CRS\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.05 ms (started: 2021-09-03 18:30:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4578gLJ_X7NQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1431b9f0-b713-44b2-ab10-adfdf462209d"
      },
      "source": [
        "data_path = \"drive/MyDrive/Datasets/MultiWOZ_2.2\"\n",
        "!mkdir -p {data_path}\n",
        "\n",
        "db_path = f\"{data_path}/db\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 116 ms (started: 2021-09-03 18:30:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baZouDZd_8wN",
        "outputId": "af726738-d697-49e3-c300-e866febdc67c"
      },
      "source": [
        "!cp drive/MyDrive/Dev/old_ID_in_CRS/Data\\ Preparation.ipynb drive/MyDrive/Dev/ID_in_CRS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 829 ms (started: 2021-09-03 18:51:49 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNPhbu6gQUri"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbQyobaY8NOp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a414adc-6e10-4b30-fe48-94925c72893b"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import glob\n",
        "import random\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.38 s (started: 2021-09-03 18:33:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH31p_IGEeXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c255268b-b1cf-4cef-f827-faa18d098be3"
      },
      "source": [
        "# !git clone https://github.com/budzianowski/multiwoz.git > /dev/null\n",
        "# !cp -r multiwoz/db {data_path}\n",
        "# !tail -n +7 {db_path}/hospital_db.json > tmp.json && mv tmp.json {db_path}/hospital_db.json && rm -rf tmp.json\n",
        "# !rm -rf multiwoz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.35 ms (started: 2021-08-27 14:19:28 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFVAbv-hyl6i",
        "outputId": "d9a31161-c2f5-4b9a-d588-434ab0b10162"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 626 ms (started: 2021-09-03 18:33:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCaEES-16sxr",
        "outputId": "11a6d625-8bfd-4491-8437-2f76e393a69c"
      },
      "source": [
        "from cleaners.mine import Cleaner\n",
        "\n",
        "cleaner = Cleaner(db_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.16 s (started: 2021-09-03 18:33:27 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii_1gLCdYyE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8165dd61-0ca4-4418-9978-eeda97f7a2db"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"multi_woz_v22\", ignore_verifications=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No config specified, defaulting to: multi_woz_v22/v2.2_active_only\n",
            "Reusing dataset multi_woz_v22 (/root/.cache/huggingface/datasets/multi_woz_v22/v2.2_active_only/2.2.0/7452f16a8b502e97df5c04cc4ee5436464762fa93b1ce778dd14181e79d8b51a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "time: 522 ms (started: 2021-08-27 14:19:28 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2OgIC_4th1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "81e5970d-4aad-4b46-f54e-df21b2520508"
      },
      "source": [
        "def parse_data(tvt):\n",
        "    filtered_domains = set([\"bus\", \"police\", \"hospital\"])\n",
        "    # filtered_domains = set()\n",
        "\n",
        "    cleaned_dialogues = {}\n",
        "    for dialogue in tqdm(dataset[tvt]):\n",
        "        dialogue_id = dialogue[\"dialogue_id\"]\n",
        "        turns = dialogue[\"turns\"]\n",
        "        cleaned_turns = []\n",
        "        for speaker, utterance, frames, dialogue_acts in zip(turns[\"speaker\"], turns[\"utterance\"], turns[\"frames\"], turns[\"dialogue_acts\"]):\n",
        "            domains, intents = [], []\n",
        "            for service, state in zip(frames[\"service\"], frames[\"state\"]):\n",
        "                if state[\"active_intent\"] != \"NONE\":\n",
        "                    domains.append(service)\n",
        "                    intents.append(state[\"active_intent\"])\n",
        "\n",
        "            if filtered_domains.intersection(domains):\n",
        "                continue\n",
        "\n",
        "            slot_positions = sorted(list(zip(dialogue_acts[\"span_info\"][\"span_start\"], dialogue_acts[\"span_info\"][\"span_end\"])))\n",
        "            slot_names0, slot_names1, slot_values = [], [], []\n",
        "            for idx1, pos in enumerate(slot_positions):\n",
        "                si, ei = pos\n",
        "                slot_value = cleaner.clean(utterance[si: ei])\n",
        "                slot_value = cleaner.tokenize(slot_value)\n",
        "                slot_name0 = dialogue_acts[\"span_info\"][\"act_slot_name\"][idx1]\n",
        "                slot_type = dialogue_acts[\"span_info\"][\"act_type\"][idx1].split('-')[0].lower()\n",
        "                # slot_type = slot_type if slot_type != \"booking\" else domains[0]\n",
        "\n",
        "                for idx2, word_slot_value in enumerate(slot_value):\n",
        "                    slot_values.append(word_slot_value)\n",
        "                    slot_names0.append(f\"{'I' if idx2 else 'B'}-{slot_name0}\")\n",
        "                    slot_names1.append(f\"{'I' if idx2 else 'B'}-{slot_type}_{slot_name0}\")\n",
        "                    # slot_names0.append(slot_name0)\n",
        "                    # slot_names1.append(f\"{slot_type}_{slot_name0}\")\n",
        "\n",
        "            text = cleaner.clean(utterance)\n",
        "            words = cleaner.tokenize(text)\n",
        "\n",
        "            slots0 = []\n",
        "            slots1 = []\n",
        "            idx = 0\n",
        "            for word in words:\n",
        "                if idx < len(slot_values) and word == slot_values[idx]:\n",
        "                    slots0.append(slot_names0[idx])\n",
        "                    slots1.append(slot_names1[idx])\n",
        "                    idx += 1\n",
        "                else:\n",
        "                    slots0.append(\"O\")\n",
        "                    slots1.append(\"O\")\n",
        "\n",
        "            if idx != len(slot_values):\n",
        "                continue\n",
        "\n",
        "            cleaned_turns.append({\n",
        "                \"speaker\": speaker,\n",
        "                \"words\": words,\n",
        "                \"slots0\": slots0,\n",
        "                \"slots1\": slots1,  # TODO: check and revise\n",
        "                \"domains\": domains,\n",
        "                \"intents\": intents\n",
        "            })\n",
        "\n",
        "        cleaned_dialogues[dialogue_id] = cleaned_turns\n",
        "\n",
        "    return cleaned_dialogues\n",
        "\n",
        "\n",
        "train_data = parse_data(\"train\")\n",
        "val_data = parse_data(\"validation\")\n",
        "test_data = parse_data(\"test\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 3895/8437 [01:50<02:08, 35.22it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-dc7c19edc3fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-dc7c19edc3fd>\u001b[0m in \u001b[0;36mparse_data\u001b[0;34m(tvt)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslot_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0msi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mslot_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutterance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mslot_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslot_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mslot_name0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdialogue_acts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"span_info\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"act_slot_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-020d331a73c4>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace_dbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace_clock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mreplace_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid_chars\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreplace_invalid_chars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-020d331a73c4>\u001b[0m in \u001b[0;36mreplace_dbs\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdb_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdb_regexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdb_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdb_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdb_items\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdb_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb_patterns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdb_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "time: 1min 50s (started: 2021-08-27 14:50:59 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcOskA91bNnQ"
      },
      "source": [
        "with open(f\"{data_path}/train.json\", \"w\") as json_file:\n",
        "    json.dump(train_data, json_file)\n",
        "with open(f\"{data_path}/validation.json\", \"w\") as json_file:\n",
        "    json.dump(val_data, json_file)\n",
        "with open(f\"{data_path}/test.json\", \"w\") as json_file:\n",
        "    json.dump(test_data, json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-gXp7GB-laA",
        "outputId": "0368ae8b-36f5-43ac-83c4-ab78c144b233"
      },
      "source": [
        "import gc\n",
        "\n",
        "del dataset\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "231"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hEbEatv9nco"
      },
      "source": [
        "# Label Refinement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtNuNcUYMCVS"
      },
      "source": [
        "import math\n",
        "import os\n",
        "import json\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import fasttext\n",
        "from collections import defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoQPFkRmbTt2"
      },
      "source": [
        "with open(f\"{data_path}/train.json\", \"r\") as json_file:\n",
        "    train_data = json.load(json_file)\n",
        "with open(f\"{data_path}/validation.json\", \"r\") as json_file:\n",
        "    val_data = json.load(json_file)\n",
        "with open(f\"{data_path}/test.json\", \"r\") as json_file:\n",
        "    test_data = json.load(json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXOLvdlBtzLw"
      },
      "source": [
        "def read_data(data):\n",
        "    X, y = [], []\n",
        "\n",
        "    for id, dlg in data.items():\n",
        "        for trn in dlg:\n",
        "            X.append(\" \".join(trn[\"words\"]))\n",
        "            \n",
        "            if len(trn[\"domains\"]):\n",
        "                y.append(1)\n",
        "            else:\n",
        "                y.append(0)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = read_data(train_data)\n",
        "X_val, y_val = read_data(val_data)\n",
        "X_test, y_test = read_data(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtIbdi8X1ibm",
        "outputId": "2915348f-6668-42fc-9c9b-1e5e4804076b"
      },
      "source": [
        "print(len(y_train), len(y_val), len(y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "107347 13928 13931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQchThbnlqeH"
      },
      "source": [
        "def prepare_fasttext_file(filename, X, Y):\n",
        "    with open(f\"{filename}.txt\", \"w\") as txt_file:\n",
        "        for x, y in zip(X, Y):\n",
        "            txt_file.write(f\"__label__{y} {x}\\n\")\n",
        "\n",
        "prepare_fasttext_file(\"train\", X_train, y_train)\n",
        "prepare_fasttext_file(\"val\", X_val, y_val)\n",
        "prepare_fasttext_file(\"test\", X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq6r187k6zwN",
        "outputId": "4d347058-f0ed-45af-b142-1be1ca87d756"
      },
      "source": [
        "import gc\n",
        "\n",
        "run_flag = False\n",
        "model_path = \"drive/MyDrive/Development/ID_in_CRS/label_model.bin\"\n",
        "\n",
        "if run_flag or not os.path.exists(model_path):\n",
        "    for rnd in range(6):\n",
        "        print(f\"ROUND {rnd + 1}\")\n",
        "\n",
        "        model = fasttext.train_supervised(\n",
        "            input=\"train.txt\",\n",
        "            autotuneValidationFile=\"val.txt\",\n",
        "            epoch=5\n",
        "        )\n",
        "\n",
        "        preds, _ = model.predict(X_test)\n",
        "        preds = [int(pred_label[0][-1]) for pred_label in preds]\n",
        "        print(classification_report(y_test, preds, digits=4))\n",
        "        print(confusion_matrix(y_test, preds))\n",
        "        print(\"#\" * 100)\n",
        "\n",
        "        num_changes = 0\n",
        "        # for X_list, y_list in [(X_train, y_train), (X_val, y_val), (X_test, y_test)]:\n",
        "        for X_list, y_list in [(X_train, y_train)]:\n",
        "            preds, probs = model.predict(X_list)\n",
        "            preds = [int(pred_label[0][-1]) for pred_label in preds]\n",
        "            probs = [prob[0] for prob in probs]\n",
        "\n",
        "            for idx in range(len(y_list)):\n",
        "                if y_list[idx] != preds[idx] and probs[idx] > (1 - (0.15 / math.log2(rnd + 2))):\n",
        "                    y_list[idx] = preds[idx]\n",
        "                    num_changes += 1  \n",
        "\n",
        "        prepare_fasttext_file(\"train\", X_train, y_train)\n",
        "        # prepare_fasttext_file(\"val\", X_val, y_val)\n",
        "        # prepare_fasttext_file(\"test\", X_test, y_test)\n",
        "        model.save_model(model_path)\n",
        "\n",
        "        del model\n",
        "        gc.collect()\n",
        "\n",
        "        if num_changes == 0:\n",
        "            break\n",
        "else:\n",
        "    model = fasttext.load_model(model_path)\n",
        "\n",
        "    preds, _ = model.predict(X_test)\n",
        "    preds = [int(pred_label[0][-1]) for pred_label in preds]\n",
        "    print(classification_report(y_test, preds, digits=4))\n",
        "    print(confusion_matrix(y_test, preds))\n",
        "\n",
        "    # for X_list, y_list in [(X_train, y_train), (X_val, y_val), (X_test, y_test)]:\n",
        "    for X_list, y_list in [(X_train, y_train)]:\n",
        "        preds, probs = model.predict(X_list)\n",
        "        preds = [int(pred_label[0][-1]) for pred_label in preds]\n",
        "        probs = [prob[0] for prob in probs]\n",
        "\n",
        "        for idx in range(len(y_list)):\n",
        "            if y_list[idx] == 1 and preds[idx] == 0 and probs[idx] > 0.95:\n",
        "                y_list[idx] = 0\n",
        "            if y_list[idx] == 0 and preds[idx] == 1 and probs[idx] > 0.95:\n",
        "                y_list[idx] = 1\n",
        "\n",
        "    prepare_fasttext_file(\"train\", X_train, y_train)\n",
        "    # prepare_fasttext_file(\"val\", X_val, y_val)\n",
        "    # prepare_fasttext_file(\"test\", X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9765    0.9858    0.9811      7875\n",
            "           1     0.9815    0.9694    0.9754      6118\n",
            "\n",
            "    accuracy                         0.9786     13993\n",
            "   macro avg     0.9790    0.9776    0.9783     13993\n",
            "weighted avg     0.9787    0.9786    0.9786     13993\n",
            "\n",
            "[[7763  112]\n",
            " [ 187 5931]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QVZm3MQ71qp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc61d63c-fb83-4701-ad8a-b5d6a3c76a90"
      },
      "source": [
        "refined_train_data, refined_val_data, refined_test_data = defaultdict(list), defaultdict(list), defaultdict(list)\n",
        "for data, y_list, refined_data in zip([train_data, val_data, test_data], [y_train, y_val, y_test], [refined_train_data, refined_val_data, refined_test_data]):\n",
        "    idx = 0\n",
        "    num_turns = 0\n",
        "    for id, dlg in data.items():\n",
        "        for trn in dlg:\n",
        "            if (len(trn[\"intents\"]) and y_list[idx] == 1) or (len(trn[\"intents\"]) == 0 and y_list[idx] == 0):\n",
        "                refined_data[id].append(trn)\n",
        "                num_turns += 1\n",
        "            idx += 1\n",
        "    print(num_turns)    \n",
        "\n",
        "train_data = refined_train_data\n",
        "# val_data = refined_val_data\n",
        "# test_data = refined_test_data\n",
        "\n",
        "del refined_train_data\n",
        "del refined_val_data\n",
        "del refined_test_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "107636\n",
            "14012\n",
            "13993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tx_LjcbC7dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a0a39c-b2de-4990-8d19-296c414366ab"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyXsxWXp_72t"
      },
      "source": [
        "# Save Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWFKQ8XkJbWd"
      },
      "source": [
        "all_words, all_domains, all_intents, all_slots0, all_slots1 = set(), set(), set(), set(), set()\n",
        "for id, dlg in train_data.items():\n",
        "    for trn in dlg:\n",
        "        all_words.update(trn[\"words\"])\n",
        "        all_domains.update(trn[\"domains\"])\n",
        "        all_intents.update(trn[\"intents\"])\n",
        "        all_slots0.update(trn[\"slots0\"])\n",
        "        all_slots1.update(trn[\"slots1\"])\n",
        "\n",
        "with open(f\"{data_path}/words.json\", \"w\") as json_file:\n",
        "    json.dump(sorted(all_words), json_file)\n",
        "with open(f\"{data_path}/domains.json\", \"w\") as json_file:\n",
        "    json.dump(sorted(all_domains), json_file)\n",
        "with open(f\"{data_path}/intents.json\", \"w\") as json_file:\n",
        "    json.dump(sorted(all_intents), json_file)\n",
        "with open(f\"{data_path}/slots0.json\", \"w\") as json_file:\n",
        "    json.dump(sorted(all_slots0), json_file)\n",
        "with open(f\"{data_path}/slots1.json\", \"w\") as json_file:\n",
        "    json.dump(sorted(all_slots1), json_file)\n",
        "\n",
        "with open(f\"{data_path}/train.json\", \"w\") as json_file:\n",
        "    json.dump(train_data, json_file)\n",
        "with open(f\"{data_path}/validation.json\", \"w\") as json_file:\n",
        "    json.dump(val_data, json_file)\n",
        "with open(f\"{data_path}/test.json\", \"w\") as json_file:\n",
        "    json.dump(test_data, json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHP14rnx8nL9"
      },
      "source": [
        "# Format to txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LT6JuzQNJ5R"
      },
      "source": [
        "import json\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRnaAOwBKt75"
      },
      "source": [
        "intent_label = \"intents\"\n",
        "slot_label = \"slots1\"\n",
        "\n",
        "for src_name, dst_name in zip([\"train\", \"validation\", \"test\"], [\"train\", \"dev\", \"test\"]):\n",
        "    idx = 0\n",
        "\n",
        "    with open(f\"{data_path}/{src_name}.json\", \"r\") as json_file:\n",
        "        data = json.load(json_file)\n",
        "\n",
        "    all_lines = []\n",
        "    for id, dialogue in data.items():\n",
        "        for turn in dialogue:\n",
        "            turn_lines = [f\"{word} {slot}\\n\" for word, slot in zip(turn[\"words\"], turn[slot_label])]\n",
        "            if len(turn[intent_label]) == 1:\n",
        "                turn_lines.append(f\"{turn[intent_label][0]}\\n\")\n",
        "                turn_lines.append(\"\\n\")\n",
        "                all_lines.extend(turn_lines)\n",
        "            # elif len(turn[intent_label]) == 0:\n",
        "            #     turn_lines.append(f\"general\\n\")\n",
        "            #     turn_lines.append(\"\\n\")\n",
        "            #     all_lines.extend(turn_lines)\n",
        "\n",
        "    with open(f\"{data_path}/{dst_name}.txt\", \"w\") as txt_file:\n",
        "        txt_file.writelines(all_lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKJuniujhNxI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}